import torch
import soundfile as sf
from univoc import Vocoder
from tacotron import load_cmudict
import matplotlib.pyplot as plt
import librosa.display
import numpy as np


# download pretrained weights (and optionally move to GPU)
vocoder = Vocoder.from_pretrained(
    "https://github.com/bshall/UniversalVocoding/releases/download/v0.2/univoc-ljspeech-7mtpaq.pt"
).cuda()

# Load CMU Pronunciation Dictionary and add pronunciation of "PyTorch"
cmudict = load_cmudict()
cmudict["PYTORCH"] = "P AY1 T AO2 R CH"

# TODO: synthesize text given mel-spectrogram
text = "A PyTorch implementation of location-relative attention mechanisms for long-form speech synthesis."

# loading wav file
filename = './data/LJ001-0001.wav'
wav, sr = librosa.load(filename)

# Generating Mel-Spectrogram
# TODO: adjust params to mimic natural speed and pitch
n_fft, n_mels, ref_db, top_db = 1024, 80, 20, 80
mel = librosa.feature.melspectrogram(
        librosa.effects.preemphasis(wav, coef=0.97),
        sr=sr, hop_length=200, win_length=800,
        n_fft=n_fft, n_mels=n_mels, fmin=50,
        norm=1, power=1,
    )
logmel = librosa.amplitude_to_db(mel, top_db=None) - ref_db
logmel = np.maximum(logmel, -top_db)
mel_ = logmel / top_db

mel = torch.tensor([mel_]).cuda()

# Synthesize the audio and generate waveform
with torch.no_grad():
    wav, sr = vocoder.generate(mel.transpose(1, 2))

# plotting the attention matrix
librosa.display.specshow(mel_,
                         sr=sr, hop_length=10, x_axis='linear')
plt.ylabel('Mel Filter')
plt.colorbar()
plt.title('LJSpeech Mel Spectrogram')

plt.savefig('./figures/LJSpeechMel.png')

# save output
sf.write(open("output/genLJSpeech_v1d0.wav", 'wb'), wav, sr)
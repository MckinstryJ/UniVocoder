import torch
import soundfile as sf
from univoc import Vocoder
from tacotron import load_cmudict, text_to_id, Tacotron
import matplotlib.pyplot as plt
from IPython.display import Audio

# download pretrained weights (and optionally move to GPU)
vocoder = Vocoder.from_pretrained(
    "https://github.com/bshall/UniversalVocoding/releases/download/v0.2/univoc-ljspeech-7mtpaq.pt"
).cuda()

tacotron = Tacotron.from_pretrained(
    "https://github.com/bshall/Tacotron/releases/download/v0.1/tacotron-ljspeech-yspjx3.pt"
).cuda()

# Load CMU Pronunciation Dictionary and add pronunciation of "PyTorch"
cmudict = load_cmudict()
cmudict["PYTORCH"] = "P AY1 T AO2 R CH"

# Text to be synthesized
text = "A PyTorch implementation of location-relative attention mechanisms for long-form speech synthesis."

# load log-Mel spectrogram from file
# mel = ...

# Synthesize the audio and generate waveform
x = torch.LongTensor(text_to_id(text, cmudict)).unsqueeze(0).cuda()
with torch.no_grad():
    mel, alpha = tacotron.generate(x)
    wav, sr = vocoder.generate(mel.transpose(1, 2))

Audio(wav, rate=sr)

# plotting the attention matrix
plt.imshow(alpha.squeeze().cpu().numpy(), vmin=0, vmax=0.8, origin="lower")
plt.xlabel("Decoder steps")
plt.ylabel("Encoder steps")
plt.savefig('./figures/change_title.png')

# save output
sf.write(open("./output/test1.wav", 'wb'), wav, sr)